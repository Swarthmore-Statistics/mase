---
title: "Mase Vignette"
author: "Iris Griffith"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{mase_vin}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(tidyverse)
#devtools::install_github("mcconvil/mase")
library(mase)
knitr::opts_chunk$set(
  fig.width = 5,
  fig.align = "center",
  cache = TRUE,
  collapse = TRUE,
  comment = "#>"
)
```
The R package `mase` contains a set of model assisted survey estimation functions for estimating population estimates, subject to a complex sampling design. In particular, `mase` offers a multitude of model assisted Generalized Regression Estimators (GREGs), as well as  other direct estimators such as the ratio estimator for variances and the post-stratified and Horvitz-Thompson (HT) estimators for population totals.

To install `mase`, simply run the `R` code: `devtools::install_github("mcconvil/mase")`, and then attach it to your library. `tidyverse` is not required by `mase`, but will be used for the purpose of this vignette:

```{r install, eval = FALSE}
library(mase)
library(tidyverse)
```

The primary dataset we will be using for this vignette is a spatially indexed satellite raster dataset of forest attributes in the Black Hills in South Dakota from the U.S. Forest Service's Geodata Clearinghouse (https://data.fs.usda.gov/geodata/rastergateway/). The population dataset contains the variables `forest_group`, `forest_nonforest_probability` and spatial grid indicies `s1` and `s2`, which represent longitudinal and latitudinal dimensions respectively. The `forest_group` variable is a categorical variable represented by integers that denote forest type codes. The codes can be found [here](https://www.fia.fs.fed.us/library/database-documentation/historic/ver3/FIADB_user%20manual_v3-0_p2_7_10_08.pdf) on appendix D, pg 222. Here, the `head()` function prints the first ten observations:

```{r load data}
data("black_hills_pop", "black_hills_samp", package = "mase")
head(black_hills_pop)
```
The sample dataset is a subset of these spatially indexed raster data, and for the purpose of this vignette, they represent a systematic random sample:
```{r}
head(black_hills_samp)
```


In addition to having the four previously mentioned variables, this sample dataset also contains a forest `biomass` (Mg/Hectare) response variable. Suppose we are interested in estimating biomass, which was measured for all of our sample observations, but not in our population level dataset. The naive way to estimate biomass would be to take the sample mean of the biomass values of our sample, but doing this does not account for the sampling bias of our systematic random sample. In order to account for this bias, we can use `mase` to employ the Horvitz-Thompson estimator and model assisted GREG, which both weight the data using the sample inclusion probabilities for a particlar sample. So suppose that our systematic sample has the following inclusion probabilities:
```{r}
inclusion_probs <- rep(c(0.0055, 0.00325, 0.00128128, 0.0072101), 58)/
  (nrow(black_hills_pop)/ nrow(black_hills_samp))
head(inclusion_probs)
```

The Horvitz-Thompson estimator can be employed by using the `horvitzThompson()` function. To use this function, simply specify a vector of sampled data and its corresponding vector of incusion probabilities:

```{r}
horvitzThompson(y = black_hills_samp$biomass, pi = inclusion_probs)
mean(black_hills_samp$biomass)
```
As we can see, our HT estimator is able to estimate a population total for biomass using these inclusion probabilities, and it can also estimate the population mean biomass using a weighted average, which we can see is different than our unweighted sample mean from the `mean()` function. The Horvitz-Thompson estimator only uses data found in our sample to build an estimate, but if an auxiliary population level dataset with variables identical to the predictors in the sample is provided, we can employ a GREG which builds a model of our response variable using our sample datasest and then projects that model across the rest of the population level data and aggregates the predictions. To do this with `mase`, we need to do a little bit of data wrangling: Since the `forest_group` variable contains hundreds of unique categories, we can break these down into more simplified softwood forest and hardwood forest categories. Since all codes greater than or equal to 400 indicate hardwood, we can easily create dummy variables for these categories. Furthermore, in order to use many of `mase`'s functions, we must be mindful to split our sample data into `y`, a vector of our response variable and `x_sample`, a dataframe of our predictor variables, and our population level data must have the same variable names as `x_sample`:

```{r}
#prepare sample data
biomass <- black_hills_samp %>%
  pull(biomass)
x_samp <- black_hills_samp %>%
  dplyr::select(forest_group, forest_nonforest_probability, s1, s2) %>%
  mutate(ponderosa = ifelse(forest_group %in% 220:224, 1, 0),
         softwood = ifelse(xor(forest_group >= 100, forest_group < 400), 1, 0)) %>%
  dplyr::select(-c("forest_group")) #drop forest group
#prepare population data
black_hills_pop <- black_hills_pop %>%
  mutate(ponderosa = ifelse(forest_group %in% 220:224, 1, 0),
         softwood = ifelse(xor(forest_group >= 100, forest_group < 400), 1, 0)) %>%
  dplyr::select(-c("forest_group")) #drop forest group
```

By providing our formatted data and our inclusion probabilities, we can finally create a GREG estimate for biomass. This first example will use an elastic net penalized linear regression model. The parameter `standardize = TRUE` standardizes the data prior to fitting a model, so that the model's coefficients are all penalized to the same scale.
```{r}
m_enet <- gregElasticNet(y = biomass, x_sample = x_samp, x_pop = black_hills_pop,
                       pi = inclusion_probs, standardize = TRUE, var_est = TRUE)
```
Running this function returns an elastic net `greg` object which summarizes the model and the population estimates for our `y` variable biomass, along with a bootstrapped variance estimate.
```{r}
m_enet
```
This `greg` object contains attributes such as:
A vector of our regression coefficients,
```{r}
m_enet$coefficients
```
A model object of class `glmnet`,
```{r, eval = F}
m_enet$model
```
Vectors for sample and population fitted values,
```{r}
m_enet$y_hat_sample %>% head()
m_enet$y_hat_pop %>% head()
```
And the means and variances of the raw sample data, if data standardization was used:
```{r}
m_enet$standardize
```

Each `greg` object also has a plot method that generates a geospatial heatmap of the fitted values of the underlying model. To plot a `greg` object, simply use the `plot` function on the `greg` object and supply the data and specify which variable names correspond to the spatial dimensions:
```{r}
plot(m_enet, y_sample = biomass, x_sample = x_samp, x_pop = black_hills_pop,
     spatial_1 = "s1", spatial_2 = "s2") +
  labs(x = "", y = "", fill = "Est. Biomass")
```
For more information about this plot method view the documentation by running the command `?plot.greg`. In addition to this plot method, each `greg` object has a `predict()` method that uses the `greg` object's model to predict response values of a new set of predictor variables.

```{r}
predict(m_enet, new_data =  black_hills_pop) %>% head()
```

In addition to linear models, `mase` also contains two regression tree based model assisted GREGs, `gregTree()` and `gregForest()`. Here we are using `gregForest()` to model forest biomass with random forests, an ensemble tree learning algorithm. The parameters of this function are similar to the parameters used in the elastic net example, but we can also specify `ntrees`, the number of trees to train in our ensemble and `mtry`, the number of predictors to randomly select for each tree.

```{r random forest}
m_forest <- gregForest(y = biomass, x_sample = x_samp, x_pop = black_hills_pop,
                       pi = inclusion_probs*(23736/232), ntrees = 300, mtry = 3)
m_forest
```

In addition to the population total and mean estimates, the random forest `greg` object also has two special attributes: An out of bag estimate for predictive mean squared error, and variable importance expressed as number of splits for a variable and the average decrease in loss (MSE) per split:
```{r}
m_forest$oob_error
m_forest$var_imp

```

And the plot and predict methods works just the same as the previous elastic net example:
```{r}
plot(m_forest, y_sample = biomass, x_sample = x_samp, x_pop = black_hills_pop,
     spatial_1 = "s1", spatial_2 = "s2") +
  labs(x = "", y = "", fill = "Est. Biomass")
predict(m_forest, x_samp) %>% head()
```

In addition to linear models and tree based regression models, `mase` contains a feed forward neural network model assisted GREG. In order to use it, you must correctly install the R `keras` package (see https://keras.rstudio.com/ for installation instructions). In order to build a neural network assisted GREG, simply specify your data, inclusion probabilities, and the neural network hyperparameters: `H` denotes the size of all three of the neural network's hidden layers, if `H` is large, the model complexity will increase and it will take longer to train. `H` should be set so that your model is neither overfitting nor underfitting the data, you can see if your model is overfitting if your validation loss winds up being larger than your training loss. `lr` is the learning rate of the gradient descent algorithm, this should be tuned so that loss between epochs descends at a smooth rate. `epochs` is the number of training epochs to run, this should be large enough to allow your loss to converge to a minimum. The default loss function used is the mean squared error between the fitted values and the observed `y` values.

```{r neural nets}
m_nn <- gregNeuralNet(y = biomass, x_sample = x_samp, x_pop = black_hills_pop,
                      pi = inclusion_probs, standardize = T, lr = .0010, epochs = 50,
                      H = 128, verbose = 1, var_est = TRUE)
m_nn
plot(m_nn, y_sample = biomass, x_sample = x_samp, x_pop = black_hills_pop,
     spatial_1 = "s1", spatial_2 = "s2")  +
  labs(x = "", y = "", fill = "Est. Biomass")
```


Lastly, `mase` also contains a radial basis function interpolated regression model assisted GREG. The advantages of this RBF model assisted GREG is that we are able to interpolate our data points so that we retain more specific knowledge of the location and spatial distribution of our estimates. The way that RBF interpolation works is much like kernel density estimation, where we add a scaled gaussian function centered at all of our data points (in the predictor space) with a fixed bandwidth. However, instead of being a gaussian distribution, these gaussian functions are scaled by a positive or negative number to meet the observed y value at that data point, almost like pitching a tent! Here, `lambda` is a hyperparameter that is inversely proportional to the bandwidth of the gaussian kernel. To tune `lambda`, you can either standardize your data and use the default value of 1.666, set it to `NULL`, in which case the `gregRBF()` function will attempt to numerically optimize `lambda`, or adjust it so that the mean estimate is close to the Horvitz-Thompson estimate of your response data, or until the spatial heatmap is congruent with your domain knowledge of the landscape. In this case, I choose `lambda = 1.9` using this latter method:

```{r RBF}
m_rbf <- gregRBF(y = biomass, x_sample = x_samp, x_pop = black_hills_pop,
                         pi = inclusion_probs, lambda = 1.9, standardize = T)
m_rbf
plot(m_rbf, y_sample = biomass, x_sample = x_samp, x_pop = black_hills_pop,
     spatial_1 = "s1", spatial_2 = "s2") +
  labs(x = "", y = "", fill = "Est. Biomass")
```






