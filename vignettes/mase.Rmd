---
title: "Model-Assisted Survey Estimators"
author: "Kelly McConville, Beck Tang, George Zhu, Shirley Cheung, Sida Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: mase.bib
vignette: >
  %\VignetteIndexEntry{Model-Assisted Survey Estimators}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r}
library(mase)
```


## Introduction

A common goal in survey sampling is to estimate finite population parameters. For instance, for a given company we might be interested in the total aggregate income of a given company as well as the mean income. Then these two quantities of interest can be written in the following manner: the total aggregate income is
$$
t_y = \sum_{i \in U} y_i,
$$
where $y_i$ respresents the income for each employer; or the mean 
$$
\mu_y = \frac{t_y}{N},
$$ 
 where $N$ is the total population size. 
 
 In the case where $y_i$ is a discrete or categorical measure (such as the marital status of the employees), we notice that $\mu_y$ would represent a proportion of those with such an attribute. 

`mase` provides several model-assisted estimators of the finite population total or mean.  All of the estimator in `mase` can be written as
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i - \hat{y}_i}{\pi_i} + \sum_{i \in U} \hat{y}_i ,
$$

where for observation $i$, $\hat{y}_i$ is the predicted value of $y$ and $\pi_i = P(i \in S)$, the inclusion probability [@cas76; @sar92]. Different models for $\hat{y}_i$ would give us different model-assisted estimators.  The estimators in `mase`, along with the assisting model, necessary auxiliary data, and addional reference, are given in the following table:

```{r, echo=FALSE, message=FALSE}
library(readr)
estimatorsTable <- read_csv("estimatorsTable.csv")
knitr::kable(estimatorsTable)
```



## Contents

1. [Data](#getting-started)
    + [Data Requirements for mase](#data-mase)
    + [Example Dataset](#example-api)
    
2. [Survey Estimators](#survey-estimators)
    + [Horvitz-Thompson Estimator](#horvitz-thompson)
    + [Post-stratified Estimator](#post-stratified)
    + [Ratio Estimator](#ratio-estimator)
    + [Generalized Regression Estimator](#linear-regression) 
    + [Elastic Net Regression Estimator](#elastic-net) 
    
## Data




### Data Requirements for mase

* Discuss the required data for each estimator.  Make a table. 
    + Column: pop data type (raw, totals/means + N)
    + Row: Estimator type
    
May not need table since logistic greg and logistic gregElasticNet are the only ones that need raw data.    

### Example Dataset

```{r, echo=FALSE, message=FALSE}
library(survey)
data(api)
options(digits=2)
```

The survey package contains a census dataset of all California schools with at least 100 students, *apipop*, and several probability samples of the data.  We will use *apisrs*, a simple random sample without replacement of size 200, and *apistrat*, a stratified random sample of size 200. In these datasets, the primary attribute of interest is the Academic Performance Index.  Additional information about the schools and demographics of the schools are provided.  

For illustration purposes we will treat *apipop* as our finite population of interest and in various examples will use the samples *apisrs* and *apistrat*.
 We will assume that the attributes of interest are only known for the sample but that the auxiliary data are known for each school.  

Assume we want to estimate ??? in insert two quantities based on api and sch.wide or comp.imp.



## Survey Estimators

This section presents guidance on when each estimator is most appropriate and shows how to fit the estimators using mase.

### Horvitz-Thompson Estimator


* **Inputs**: 
* **Outputs**: 

If no auxiliary data are available, then $\hat{y}_i = 0$ in the model-assisted estimator of $t_y$ and the estimator simplifies to
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i}{\pi_i},
$$
the survey-weighted sum of the sampled values [@hor52].  This estimator can be fit in mase using the following code:

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1))


ht_strat <- horvitzThompson(y = apistrat$api00, pi = apistrat$pw^(-1))

```

Then the Horvitz-Thompsons estimator for the average score is `r ht_srs$pop_mean` for the simple random sample and `r ht_strat$pop_mean` for the stratified sample.  Since $N$, the population size, was not specified, it was assumed to be $\sum_{i \in s} \pi_i^{-1}$.

The variance estimator can be found by adding the argument `var_est = TRUE` and specifying a method with `var_method`.

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")

```

For the simple random sample, the variance of the HT estimator of the mean score is `r ht_srs$pop_mean_var`.  If the joint inclusions probabilites are known and positive (argument `pi2`), then the exact Horvitz-Thompson estimator of other single stage sampling designs can be found.  The variance can be estimated by selected one of the with-replacement variance estimators.

```{r}
ht_strat <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HB")
```


### Post-stratified Estimator 


It is not uncommon for us to have categorical auxilary data that might give information we would like to use in modelling. It has been shown that such information can often increase the estimator's efficiency in that it could increase the precision of our estimates by reducing variance. For instance, that a particular location is covered by forest would give good predictor of canopy cover since the plots of land labelled "forest"" is much more likely to be convered by canopy than those that are labelled "non-forest".



We can compute the mean of each population and the variance of the overall population. We first give general notations: let $y^h_j$ denote the value of the $j$-th unit in the category $h$; let

$$
t_j = \sum _{i=1} ^ {N_j} x_{ij}
$$

denote the sum of relevant interest in category $j$ (where in this context $j$ would take on two values to indicate whether the plot is covered by forest or not). Then we have that the sample mean in stratum $j$  is 

$$
\overline{y}_j = \frac{\sum_{i\in S_j} x_{ij}}{n_j},
$$

where the summation extends to all the samples in the stratum $h$ of our sample, and $n_h$ is the population of the stratum $h$. Note that the non-capitalized $n$ stands for population size of the sample. 

Suppose $y_i$ stands for canopy cover in plot $i$. The above could be modeled as @sar92
$$
y_i = \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i. 
$$
Assuming constant variance of the error term $\epsilon_i$ and making use of the HT estimator, we can give the following estimates :
$$
\hat{\beta}_1 =\frac{ \sum _{i \in S_1} \frac{y_i}{\pi_i}}{\hat{N_1}},\\
\hat{\beta}_2 =\frac{ \sum _{i \in S_2} \frac{y_i}{\pi_i}}{\hat{N_2}},
$$
from which we could derive the post-stratified mean 
$$
\hat{\mu} = \frac{1}{N} \sum _{j=1}^2 \frac{N_j}{\hat{N}_j} \sum _{i \in S_j} \frac{y_i}{\pi_i}.
$$
If we have a simple random sampling, then estimated variance could be written as
$$
\hat{V}_{\text{post}} \approx (1-\frac{n}{N}) \sum^H _ {h=1} \frac{N_h}{N} \frac{s^2_h}{n},
$$
where $s^2_h$ is the sample variance in stratum $h$.

As it turns out, the desired quantities $N_h$ and $N$ are frequently unavailable, and we would like to consider models where we have will need to break assumptions of simple random sampling. 

(To be continued)

In our example dataset, the binary categorical variable `awards` specifies whether a school is eligible for an awards program.   From the graph below, it appears that eligible schools have, on average, a higher api score.

```{r, echo=FALSE}
library(ggplot2)
ggplot(apistrat, aes(x = awards, y = api00)) + geom_boxplot()+ labs(x="Eligible for Awards Program?", y="API Scores for 2000") + theme_bw() 
```

To add the awards information, we need to include the sample values (`xsample`) and the population data (`xpop`). The following R scripts show three ways f achieving this.

```{r, message=FALSE}

#Raw data
ps_srs_r <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = apipop$awards, datatype = "raw", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")


#Totals in each category 
pop_tots <- data.frame(table(apipop$awards))
ps_srs_t <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_tots, datatype = "totals", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")

#Means in each category
pop_means <- data.frame(table(apipop$awards)/length(apipop$awards))
ps_srs_m <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_means, datatype = "means", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")
```



`r ps_srs_r$pop_mean`

### Ratio Estimator
The presence of auxilary data would sometimes allow us to compute ratios of interest that will be of significant aid to the model. In particular, the ratio for our estimated quantity of interest $y_i$ and an auxilary predictor $x_i$ might have a fairly constant ratio $\frac{y_i}{x_i}$ throughout all samples $i\in S$. Suppose that this ratio has average $\beta $ that stays fairly constant throughout our samples. Then we can give the model 
$$
y_i = \beta x_i + \epsilon_i, 
$$
where the errors $\epsilon_i$ would structurally have variance $\sigma ^2 x_i$ and mean at 0.

In light of our HT estimator of $\mu_y$ and of $\mu_x$, we would have our estimated
$$
\hat{\beta}= \frac{\hat {\mu}_{y,ht}}{\hat{\mu}_{x,HT}}.
$$
It can then be shown from GREG estimator below that 
$$
\hat{\mu_y} = 0 + \frac{\mu_x}{\hat{\mu}_{x, HT}} \hat{\mu}_{y,ht,}
$$
to which we dub the name ratio estimator. @sar92.

(Code examples to follow)


### Generalized Regression Estimator 
The generalized regression estimator (GREG) can be used when the auxiliary data includes a mixture of both quantitative and categorical variables. It is expressed in the following form:
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}-\hat{m}(\boldsymbol{x}_{i})}{\pi_{i}} 
                + \sum_{i \in U}\hat{m}(\boldsymbol{x}_{i}) \right)
$$
where $\boldsymbol{x}_{i}$ is the auxiliary data, $\hat{m}(\boldsymbol{x}_{i})$ is the predicted value of $y$ given the auxiliary data, $y_{i}$ is the observed, $\pi_{i}$ is the inclusion probability, and $N$ is the size of the finite population. 

The exact form of the GREG is flexible. It varies depending on whether $y$ is quantitative or categorical and the availablity and relevancy of auxiliary data. Note that all aforementioned estimators are variations of the GREG. For example, in the case that no relevant auxiliary data exist, the $\hat{m}(x_{i})$ portions of our GREG equation simply equal zero, resulting in our Horvitz-Thompson estimator: 
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}}{\pi_{i}} \right)
$$

-- INSERT COEFFICIENT CALCULATION HERE -- 

When $y$ is quantitative, GREG is called a linear regression as follows:
$$
y_{i} = \beta_{o} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + ... + \beta_{p}x_{ip} + 
        \epsilon_{i} \\
      = \boldsymbol{x}_{i}^T \boldsymbol{\beta} + \epsilon_{i}
$$

The following code calculates GREG for the quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
greg_linear_srs <- greg(y = apisrs$api00, 
                        xsample = apisrs[c("col.grad", "awards")], 
                        xpop = apipop[c("col.grad", "awards")], 
                        pi = apisrs$pw^(-1), model = "linear",
                        var_est = TRUE)
greg_linear_srs

# GREG applied to stratified sample
greg_linear_strat <- greg(y = apistrat$api00, 
                          xsample = apistrat[c("col.grad", "awards")],
                          xpop = apipop[c("col.grad", "awards")],
                          pi = apistrat$pw^(-1), model = "linear",
                          var_est = TRUE)
greg_linear_strat
```

When $y$ is categorical, GREG is called a logistic regression as follows:
$$
P(y_{i} = 1|\boldsymbol{x}_{i}) = \frac{exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
                                  {1+exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
$$

The logistic model results in probabilities between 0 and 1. The following code calculates GREG for the categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
# categorical with more than two categories, emit warning message, return 

# Initialize new columns
apisrs[,"grad"] <- NA
for (i in length(apisrs$col.grad)) {
  if()
}


# convert yes/no into 1 and 0
greg_logistic_srs <- greg(y = apisrs$awards, 
                          xsample = apisrs[c("col.grad", "sch.wide")],
                          xpop = apipop[c("col.grad", "sch.wide")], 
                          pi = apisrs$pw^(-1), model = "logistic"
                          )
greg_logistic_srs

# GREG applied to stratified sample
greg_logistic_strat <- greg(y = apistrat$awards, 
                            xsample = apistrat[c("col.grad", "sch.wide")],
                            xpop = apipop[c("col.grad", "sch.wide")],
                            pi = apistrat$pw^(-1), model = "logistic")
greg_logistic_strat
```

### Elastic Net Regression Estimator
The Elastic Net Regression estimator is simply the GREG estimator above, with an added condition in the original formula which uses a penalized survey-weighted least squares to incorporate model selection when estimating our coefficients. This added condition is called the "elastic net." 
$$
\hat{\boldsymbol{\beta}_{s}} = \underset{\boldsymbol{\beta}}{\text{arg min}} 
                              \left \{\sum_{i \in s}
                              \frac{(y_{i}-\boldsymbol{x}_{i}^T \boldsymbol{\beta})^2}
                              {\pi_{i}\sigma^2_{i}} + \lambda
                              \left[\alpha\sum_{j=1}^p \left|\beta_{j} \right| + 
                              (1-\alpha)\sum_{j=1}^p \beta^2_{j} \right]
                              \right \}
$$
Observe that the first half of the equation is the original GREG formula. The second half of the equation is the aforementioned "elastic net" condition, which is the penalty calculation.The elastic net itself is split into two halves. The first half of the elastic net, $\alpha\sum_{j=1}^p \left|\beta_{j} \right|$, is known as the LASSO penalty, which shrinks the amount of unnecessary variables. The second half of the elastic net, $(1-\alpha)\sum_{j=1}^p \beta^2_{j}$, is the Ridge Regression penalty, which is good for multicollinearity. The alpha value decides the relative amount weight to assign to each type of penalty. 

At this point, one might wonder - why add a penalty to the original GREG formula? Why create an elastic net regression estimator? What is "better" about the elastic net regression estimator than the generalized regression estimator? The GREG estimator does not guard against the presence of too many auxiliary variables, which may result in added variability. With the addition of the elastic net condition, the elastic net regression estimator shrinks the coefficients towards zero, therefore resulting in a better variability than the GREG. To combat the presence of superfluous auxiliary data, one could also choose subsets on which to perform GREG, but this process is tedious and time consuming, as well as inefficient. Imagine how many combinations can be made from n choose 2, n choose 3, n choose 4, etc. number of subsets! Therefore, by adding an elastic net condition to our original GREG estimator, we allow the penalization condition to "choose" the important $\beta$'s for us. 

Likewise with GREG, one can calculate linear and logistic regressions using the Elastic Net estimator. The following code calculates the elastic net regression for our quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package.
```{r}
# Linear Elastic Net applied to simple random sample
# give long x vector to show how elastic net will "discard" certain variables
gregElastic_lin_srs <- gregElasticNet(y = apisrs$api00, 
                                      xsample = apisrs[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "comp.imp", "meals", 
                                                         "sch.wide")],
                                      xpop = apipop[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "comp.imp", "meals", 
                                                         "sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_srs

# Linear Elastic Net applied to stratified sample
gregElastic_lin_strat <- gregElasticNet(y = apistrat$api00, 
                                      xsample = apistrat[c("col.grad", "awards")],
                                      xpop = apipop[c("col.grad", "awards")],
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_strat
```

talk about discarded variables
talk about std changes vs. just greg 

The following code calculates the elastic net regression for our categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package.
```{r}
# Logistic Elastic Net applied to simple random sample
# give long x vector to show how elastic net will "discard" certain variables
gregElastic_log_srs <- gregElasticNet(y = apisrs$api00, 
                                      xsample = apisrs[c("col.grad", "sch.wide")],
                                      xpop = apipop[c("col.grad", "sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_srs

# Logistic Elastic Net applied to stratified sample
gregElastic_log_strat <- gregElasticNet(y = apistrat$api00, 
                                      xsample = apistrat[c("col.grad", "sch.wide")],
                                      xpop = apipop[c("col.grad", "sch.wide")],
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_strat
```

talk about discarded variables
talk about std changes vs. just greg 

update greg with beta calculations
update function



### References