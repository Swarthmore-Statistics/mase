---
title: "Model-Assisted Survey Estimators"
author: "Kelly McConville, Beck Tang, George Zhu, Shirley Cheung, Sida Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: mase.bib
vignette: >
  %\VignetteIndexEntry{Model-Assisted Survey Estimators}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r}
library(mase)
```


## Introduction

A common goal in survey sampling is to estimate finite population parameters. For instance, for a given company we might be interested in the total aggregate income of a given company as well as the mean income. Then these two quantities of interest can be written in the following manner: the total aggregate income is
$$
t_y = \sum_{i \in U} y_i,
$$
where $y_i$ respresents the income for each employee; or the mean 
$$
\mu_y = \frac{t_y}{N},
$$ 
 where $N$ is the total population size. In this example $N$ refers to the total number of employees. 
 
 In the case where $y_i$ is a discrete or categorical measure (such as the marital status of the employees), we notice that $\mu_y$ would represent a proportion of those with such an attribute. 

`mase` provides several model-assisted estimators of the finite population total or mean.  All of the estimator in `mase` can be written as
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i - \hat{y}_i}{\pi_i} + \sum_{i \in U} \hat{y}_i ,
$$

where for observation $i$, $\hat{y}_i$ is the predicted value of $y$ and $\pi_i = P(i \in s)$, the inclusion probability [@cas76; @sar92]. Different models for $\hat{y}_i$ result in different model-assisted estimators.  The estimators in `mase`, along with the assisting model, necessary auxiliary data, and additional reference, are given in the following table:

```{r, echo=FALSE, message=FALSE}
library(readr)
estimatorsTable <- read_csv("estimatorsTable.csv")
knitr::kable(estimatorsTable)
```



## Contents

1. [Data](#getting-started)
    + [Data Requirements for mase](#data-mase)
    + [Example Dataset](#example-api)
    
2. [Survey Estimators](#survey-estimators)
    + [Horvitz-Thompson Estimator](#horvitz-thompson)
    + [Post-stratified Estimator](#post-stratified)
    + [Ratio Estimator](#ratio-estimator)
    + [Generalized Regression Estimator](#linear-regression) 
    + [Elastic Net Regression Estimator](#elastic-net) 
    
## Data




### Data Requirements for mase (How much of the original discussion should be kept? )


* Discuss the required data for each estimator.  Make a table. 
    + Column: pop data type (raw, totals/means + N)
    + Row: Estimator type
    
May not need table since logistic greg and logistic gregElasticNet are the only ones that need raw data.  

Paragraph on auxilary data: There are a couple of common forms in which we have auxilary data. The first form is where we are given concrete observation regarding the information pertaining to each object (in the RAW) form. The second form is in a summarized/analyzed data, where we might have either the total counts of a variable of interest or the relevant mean value. In both cases we would be additionally supplied with the number of observations/objects within the sample/population.

We will note that, of the estimators we will introduce in this documentation, the Horvitz Thompson estimators does not make use of auxilary data, and with the exception of the logistic greg estimators and its counterpart using elastic net, our estimators can take in all data forms--raw, total counts and with population size, and mean counts with population size.

### Example Dataset

```{r, echo=FALSE, message=FALSE}
library(survey)
data(api)
options(digits=2)
```

The survey package contains a census dataset of all California schools with at least 100 students, *apipop*, and several probability samples of the data.  We will use *apisrs*, a simple random sample without replacement of size 200, and *apistrat*, a stratified random sample of size 200. In these datasets, the primary attribute of interest is the Academic Performance Index.  Additional information about the schools and demographics of the schools are provided.  

For illustration purposes we will treat *apipop* as our finite population of interest and in various examples will use the samples *apisrs* and *apistrat*.
 We will assume that the attributes of interest are only known for the sample but that the auxiliary data are known for each school.  

#What do we make use of this sentence?
Assume we want to estimate ??? in insert two quantities based on api and sch.wide or comp.imp.



## Survey Estimators

This section presents guidance on when each estimator is most appropriate and shows how to fit the estimators using mase.

### Horvitz-Thompson Estimator

If no auxiliary data are available, then $\hat{y}_i = 0$ in the model-assisted estimator of $t_y$ and the estimator simplifies to
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i}{\pi_i},
$$
the survey-weighted sum of the sampled values [@hor52].  To give an estimate of the average score, we use the HT estimator in the following manner:

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1))
ht_srs

ht_strat <- horvitzThompson(y = apistrat$api00, pi = apistrat$pw^(-1))
ht_strat
```

Then the Horvitz-Thompsons estimator for the average score is `r ht_srs$pop_mean` for the simple random sample and `r ht_strat$pop_mean` for the stratified sample. Since $N$, the population size, was not specified, it was assumed to be $\sum_{i \in s} \pi_i^{-1}$.
Note that we specify the inclusion probabilities with argument "pi".  If "pi" is unavailable or not supplied, the function would require the total population size $N$, and assume simple random sampling. 
```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, N = length(apisrs$api00))
ht_srs
#Is this output actually correct? 
```

The variance estimator can be found by adding the argument `var_est = TRUE` and specifying a method with `var_method`.

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")
```

For the simple random sample, the variance of the HT estimator of the mean score is `r ht_srs$pop_mean_var`.  If the joint inclusions probabilites are known and positive (argument `pi2`), then the exact Horvitz-Thompson estimator of other single stage sampling designs can be found.  The variance can be estimated by selected one of the with-replacement variance estimators. 

An introduction to the various variance estimators is given at the last section of this documentaiton. 

```{r}
ht_strat <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HB")
```

### Post-stratified Estimator 


It is not uncommon for us to have categorical auxilary data that might give information we would like to use in modeling. It has been shown that such information can often increase the precision of our estimates by reducing variance. For instance, that a particular location is covered by forest would give good predictor of canopy cover since the plots of land labelled "forest"" is much more likely to be covered by canopy than those that are labelled "non-forest".

For instance, suppose we want to estimate $\mu_y$, where $y_i$ denotes the percentage of canopy cover for unit $i$ and we have a single categorical $x$ variable with known number of Categories.Then, assuming the group mean model  
$$
y_i = \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i, 
$$
the generalized regression estimator simplifies to the post-stratified estimator
$$
#Fix the upper bound of the first summand (it's not 2; what is it?)
\hat{\mu_j} = \frac{1}{N} \sum _{j=1}^2 \frac{N_j}{\hat{N}_j} \sum _{i \in S_j} \frac{y_i}{\pi_i},
$$
#Where $S_j $ refers to the sample (what j? Stratum j? What is it?) 

In particular, we note that the quantity $\frac{1}{\hat{N}_j} \sum _{}$
If we have a simple random sampling, then estimated variance could be written as
$$
\hat{V}_{\text{post}} \approx (1-\frac{n}{N}) \sum^H _ {h=1} \frac{N_h}{N} \frac{s^2_h}{n},
$$
where $s^2_h$ is the sample variance in stratum $h$.

As it turns out, the desired quantities $N_h$ and $N$ are frequently unavailable, and we would like to consider models where we have will need to break assumptions of simple random sampling. 

Mick: Investigate the variance structure of post-stratified without simple random sampling. Consult big-fat-blue stat book.


In our example dataset, the binary categorical variable `awards` specifies whether a school is eligible for an awards program.   From the graph below, it appears that eligible schools have, on average, a higher api score.

```{r, echo=FALSE}
library(ggplot2)
ggplot(apistrat, aes(x = awards, y = api00)) + geom_boxplot()+ labs(x="Eligible for Awards Program?", y="API Scores for 2000") + theme_bw() 
```

To add the awards information, we need to include the sample values (`xsample`) and the population data (`xpop`). The following R scripts show three ways of achieving this, corresponding to three common forms in which we might be given the data. The first form of data we might be given is a detailed list of the state of all observations of the categorical variables describing whether they belong to this category. This data form is called "raw" data, and the following code shows how we can perform relevant computation.

```{r, message=FALSE}

#Raw data
#This method does not need to first extract population totals or population mean 
#(I need to explain why this method is sometimes unavailable. When do people not use this method?)
ps_srs_r <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = apipop$awards, datatype = "raw", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "bootstrapSRS")
```
The second form in which data often given is in terms of counts of observations in one cateogry and the total of those not in this category. The code will first produce data in such a format, stored in "pop_totals", and then perform computation based on this given information.

Now suppose that we don't have the raw data; we can achieve this through other information that is often provided
```{r, message=FALSE}
#Totals in each category 
#First computes the total number of awards, thus mimicking the form of given totals
pop_totals <- data.frame(table(apipop$awards))
ps_srs_t <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_totals, datatype = "totals", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")
```

The third form of data is one where we have the proportion, or the percetange of observations in or out of each category. Again, the first line of code gives such a summary of data, and computation is performed by code given on the second line. 
```{r, message=FALSE}
#Means in each category
#First computes the proportion of the two categories
pop_means <- data.frame(table(apipop$awards)/length(apipop$awards))
ps_srs_m <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_means, datatype = "means", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HT")
```


`r ps_srs_r$pop_mean`

### Ratio Estimator
The presence of auxilary data would sometimes allow us to compute ratios of interest that will be of significant aid to the model. In particular, the ratio for our estimated quantity of interest $y_i$ and an auxilary predictor $x_i$ might have a fairly constant ratio $\frac{y_i}{x_i}$ throughout all samples $i\in S$. Suppose that this ratio has average $\beta $ that stays fairly constant throughout our samples. Then we can give the model 
$$
y_i = \beta x_i + \epsilon_i, 
$$
where the errors $\epsilon_i$ would structurally have variance $\sigma ^2 x_i$ and mean at 0.

In light of our HT estimator of $\mu_y$ and of $\mu_x$, we would have our estimated
$$
\hat{\beta}= \frac{\hat {\mu}_{y,ht}}{\hat{\mu}_{x,HT}}.
$$
It can then be shown from GREG estimator below that 
$$
\hat{\mu_y} = 0 + \frac{\mu_x}{\hat{\mu}_{x, HT}} \hat{\mu}_{y,ht,}
$$
to which we give the name ratio estimator. @sar92.

We should nevertheless note a couple of caveats for the ratio estimator. Ratio Estimators are often biased. (It assumes regression through the origin which might contribute to the bias.)

Ratio estimation is most appropriate if a straight line through the origin summarizes the relationship between xi and yi and if the variance of yi about the line is proportional to xi. This assumption is often quite difficult to meet; even in simple random sampling where we already have that the ratio between yi and xi are constant, very little can we say about whether the regression line between these two variables traverse the origin.

We now give examples of using the ratio estimator to compute population mean.


We first perform some data exploration.
```{r}
explor_quant <- apisrs[,c(12, 20:23)]
pairs(explor_quant)
```
From this plot above, it seems that a good predictor for api00 is meals, since the negative linear correlation seems strong. 
```{r, message=FALSE}

#Raw data
#Just need to feed in one variable!!
re_srs_raw <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = apipop$meals, datatype = "raw", pi = NULL, N = NULL, pi2= NULL, var_est = TRUE, var_method = "HTSRS", B = 1000)
re_srs_raw
pop_means <- mean(apipop$meals)
re_srs_means <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_means, datatype = "means", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "HB", B = 1000)
re_srs_means
pop_totals <- sum(apipop$meals)
re_srs_totals <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_totals, datatype = "totals", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "bootstrapSRS", B = 1000)
re_srs_totals
```
Note that the three estimates agree completely with each other, as we should expect. We can modify our method according to the layout of our dataset; we can change which variance estimator to use in the "var" option.
### Generalized Regression Estimator 
The generalized regression estimator (GREG) can be used when the auxiliary data includes a mixture of both quantitative and categorical variables. It is expressed in the following form:
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}-\hat{m}(\boldsymbol{x}_{i})}{\pi_{i}} 
                + \sum_{i \in U}\hat{m}(\boldsymbol{x}_{i}) \right)
$$
where $\boldsymbol{x}_{i}$ is the auxiliary data, $\hat{m}(\boldsymbol{x}_{i})$ is the predicted value of $y$ given the auxiliary data, $y_{i}$ is the observed, $\pi_{i}$ is the inclusion probability, and $N$ is the size of the finite population. 

The exact form of the GREG is flexible. It varies depending on whether $y$ is quantitative or categorical and the availablity and relevancy of auxiliary data. Note that all aforementioned estimators are variations of the GREG. For example, in the case that no relevant auxiliary data exist, the $\hat{m}(x_{i})$ portions of our GREG equation simply equal zero, resulting in our Horvitz-Thompson estimator: 
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}}{\pi_{i}} \right)
$$


When $y$ is quantitative, GREG is called a linear regression as follows:
$$
y_{i} = \beta_{o} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + ... + \beta_{p}x_{ip} + 
        \epsilon_{i} \\
      = \boldsymbol{x}_{i}^T \boldsymbol{\beta} + \epsilon_{i}
$$

The following code calculates GREG for the quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
greg_linear_srs <- greg(y = apisrs$api00, 
                        xsample = apisrs[c("col.grad", "awards")], 
                        xpop = apipop[c("col.grad", "awards")], 
                        pi = apisrs$pw^(-1), model = "linear",
                        var_est = TRUE)
greg_linear_srs

# GREG applied to stratified sample
greg_linear_strat <- greg(y = apistrat$api00, 
                          xsample = apistrat[c("col.grad", "awards")],
                          xpop = apipop[c("col.grad", "awards")],
                          pi = apistrat$pw^(-1), model = "linear",
                          var_est = TRUE)
greg_linear_strat
```

When $y$ is categorical, GREG is called a logistic regression as follows:
$$
P(y_{i} = 1|\boldsymbol{x}_{i}) = \frac{exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
                                  {1+exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
$$

The logistic model results in probabilities between 0 and 1. The following code calculates GREG for the categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
# categorical with more than two categories, emit warning message, return 

# For categorical variables, we need to convert yes/no to 1/0. In our logistic regression, our categorical variables are "awards" and "sch.wide." We need to convert these columns into 1/0s in both the srs, strat, and the population 

######### SRS #########
# Creating a new column to contain 1/0s
apisrs[c("new_sch.wide")] <- NA
apisrs[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:200) {
  if(apisrs$sch.wide[i] == "Yes" ) {
    apisrs$new_sch.wide[i] <- 1
  } else if (apisrs$sch.wide[i] == "No") {
    apisrs$new_sch.wide[i] <- 0
  } 
}

# Converting "awards" from yes/no to 1/0
for (i in 1:200) {
  if (apisrs$awards[i] == "Yes") {
    apisrs$new_awards[i] <- 1
  } else if (apisrs$awards[i] == "No") {
    apisrs$new_awards[i] <- 0
  }
}

######### STRAT #########
# Creating a new column to contain 1/0s
apistrat[c("new_sch.wide")] <- NA
apistrat[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:200) {
  if(apistrat$sch.wide[i] == "Yes") {
    apistrat$new_sch.wide[i] <- 1
  } else if (apistrat$sch.wide[i] == "No") {
    apistrat$new_sch.wide[i] <- 0
  }
}

# Converting "awards" from yes/no to 1/0
for (i in 1:200) {
  if(apistrat$awards[i] == "Yes") {
    apistrat$new_awards[i] <- 1
  } else if (apistrat$awards[i] == "No") {
    apistrat$new_awards[i] <- 0
  }
}

######### POPULATION #########
# Creating a new column to contain 1/0s
apipop[c("new_sch.wide")] <- NA
apipop[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:6194) {
  if(apipop$sch.wide[i] == "Yes") {
    apipop$new_sch.wide[i] <- 1
  } else if (apipop$sch.wide[i] == "No") {
    apipop$new_sch.wide[i] <- 0
  }
}

# Converting "awards" from yes/no to 1/0
for (i in 1:6194) {
  if(apipop$awards[i] == "Yes") {
    apipop$new_awards[i] <- 1
  } else if (apipop$awards[i] == "No") {
    apipop$new_awards[i] <- 0
  }
}

######### GREG Calculations #########
# GREG applied to simple random sample
greg_logistic_srs <- greg(y = apisrs$new_awards, 
                          xsample = apisrs[c("col.grad", "new_sch.wide")],
                          xpop = apipop[c("col.grad", "new_sch.wide")], 
                          pi = apisrs$pw^(-1), model = "logistic")
greg_logistic_srs

# GREG applied to stratified sample
greg_logistic_strat <- greg(y = apistrat$new_awards, 
                            xsample = apistrat[c("col.grad", "sch.wide")],
                            xpop = apipop[c("col.grad", "sch.wide")],
                            pi = apistrat$pw^(-1), model = "logistic")
greg_logistic_strat
```

### Elastic Net Regression Estimator
The Elastic Net Regression estimator is simply the GREG estimator above, with an added condition in the original formula which uses a penalized survey-weighted least squares to incorporate model selection when estimating our coefficients. This added condition is called the "elastic net." 
$$
\hat{\boldsymbol{\beta}_{s}} = \underset{\boldsymbol{\beta}}{\text{arg min}} 
                              \left \{\sum_{i \in s}
                              \frac{(y_{i}-\boldsymbol{x}_{i}^T \boldsymbol{\beta})^2}
                              {\pi_{i}\sigma^2_{i}} + \lambda
                              \left[\alpha\sum_{j=1}^p \left|\beta_{j} \right| + 
                              (1-\alpha)\sum_{j=1}^p \beta^2_{j} \right]
                              \right \}
$$
Observe that the first half of the equation is the original GREG formula. The second half of the equation is the aforementioned "elastic net" condition, which is the penalty calculation.The elastic net itself is split into two halves. The first half of the elastic net, $\alpha\sum_{j=1}^p \left|\beta_{j} \right|$, is known as the LASSO penalty, which shrinks the amount of unnecessary variables. The second half of the elastic net, $(1-\alpha)\sum_{j=1}^p \beta^2_{j}$, is the Ridge Regression penalty, which is good for multicollinearity. The alpha value decides the relative amount weight to assign to each type of penalty. 

At this point, one might wonder - why add a penalty to the original GREG formula? Why create an elastic net regression estimator? What is "better" about the elastic net regression estimator than the generalized regression estimator? The GREG estimator does not guard against the presence of too many auxiliary variables, which may result in added variability. With the addition of the elastic net condition, the elastic net regression estimator shrinks the coefficients towards zero, therefore resulting in a better variability than the GREG. To combat the presence of superfluous auxiliary data, one could also choose subsets on which to perform GREG, but this process is tedious and time consuming, as well as inefficient. Imagine how many combinations can be made from n choose 2, n choose 3, n choose 4, etc. number of subsets! Therefore, by adding an elastic net condition to our original GREG estimator, we allow the penalization condition to "choose" the important $\beta$'s for us. 

Likewise with GREG, one can calculate linear and logistic regressions using the Elastic Net estimator. The following code calculates the elastic net regression for our quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package.
```{r}
# Linear Elastic Net applied to simple random sample
# give long x vector to show how elastic net will "discard" certain variables
gregElastic_lin_srs <- gregElasticNet(y = apisrs$api00, 
                                      xsample = apisrs[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "sch.wide")],
                                      xpop = apipop[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_srs

# Linear Elastic Net applied to stratified sample
gregElastic_lin_strat <- gregElasticNet(y = apistrat$api00, 
                                      xsample = apistrat[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "sch.wide")],
                                      xpop = apipop[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "sch.wide")], 
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_strat
```

talk about discarded variables
talk about std changes vs. just greg 

The following code calculates the elastic net regression for our categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package.
```{r}
# Logistic Elastic Net applied to simple random sample
gregElastic_log_srs <- gregElasticNet(y = apisrs$new_awards, 
                                      xsample = apisrs[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "new_sch.wide")],
                                      xpop = apipop[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "new_sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_srs

# Logistic Elastic Net applied to stratified sample
gregElastic_log_strat <- gregElasticNet(y = apistrat$new_awards, 
                                      xsample = apistrat[c("col.grad",  
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "new_sch.wide")],
                                      xpop = apipop[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "new_sch.wide")], 
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_strat
```

talk about discarded variables
talk about std changes vs. just greg 

##Variance Estimation
-Talk about all variance estimators in the varMase.R; talk heuristically (What information is needed; )
Show example by computing via different methods
-Bootsrap


### References