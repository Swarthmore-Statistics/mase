---
title: "Model-Assisted Survey Estimators"
author: "Kelly McConville, Beck Tang, George Zhu, Shirley Cheung, Sida Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: mase.bib
vignette: >
  %\VignetteIndexEntry{Model-Assisted Survey Estimators}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r}
library(mase)
```



## Contents

1. [Introduction](#intro)
1. [Data](#getting-started)
    + [Data Requirements for mase](#data-mase)
    + [Example Dataset](#example-api)
    
2. [Survey Estimators](#survey-estimators)
    + [Horvitz-Thompson Estimator](#horvitz-thompson)
    + [Post-stratified Estimator](#post-stratified)
    + [Ratio Estimator](#ratio-estimator)
    + [Generalized Regression Estimator](#linear-regression) 
    + [Elastic Net Regression Estimator](#elastic-net) 
    

## Introduction

A common goal in survey sampling is to estimate finite population parameters. For instance, we might be interested in either the total or the average hours spent on social media for residents of a particular state. These two quantities of interest can be written in the following manner: the total hours is
$$
t_y = \sum_{i \in U} y_i,
$$
where $y_i$ respresents the number of hours for the $i$-th resident in the enumerated list of residents, $U$; or the mean 
$$
\mu_y = \frac{t_y}{N},
$$ 
 where $N$ is the total population size. In this example $N$ refers to the total number of residents in the state. 
 
 In the case where $y_i$ is a discrete or categorical measure (such as whether or not a resident has a Twitter account), we notice that $\mu_y$ would represent a proportion of those with such an attribute. 

`mase` provides several model-assisted estimators of the finite population total or mean.  All of the estimator in `mase` can be written as
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i - \hat{y}_i}{\pi_i} + \sum_{i \in U} \hat{y}_i ,
$$

where for observation $i$, $\hat{y}_i$ is the predicted value of $y$ based on some model and $\pi_i = P(i \in s)$, the probability of inclusion in the sample [@cas76; @sar92]. Different models for $\hat{y}_i$ result in different model-assisted estimators.  The estimators in `mase`, along with the assisting model, necessary auxiliary data, and additional reference, are given in the following table:

```{r, echo=FALSE, message=FALSE}
library(readr)
estimatorsTable <- read_csv("estimatorsTable.csv")
knitr::kable(estimatorsTable)
```



## Data




### Data Requirements for mase (How much of the original discussion should be kept? )


* Discuss the required data for each estimator.  Make a table. 
    + Column: pop data type (raw, totals/means + N)
    + Row: Estimator type
    
May not need table since logistic greg and logistic gregElasticNet are the only ones that need raw data.  

Paragraph on auxilary data: There are a couple of common forms in which we have auxilary data. The first form is where we are given concrete observation regarding the information pertaining to each object (in the RAW) form. The second form is in a summarized/analyzed data, where we might have either the total counts of a variable of interest or the relevant mean value. In both cases we would be additionally supplied with the number of observations/objects within the sample/population.

We will note that, of the estimators we will introduce in this documentation, the Horvitz Thompson estimators does not make use of auxilary data, and with the exception of the logistic greg estimators and its counterpart using elastic net, our estimators can take in all data forms--raw, total counts and with population size, and mean counts with population size.

### Example Dataset

```{r, echo=FALSE, message=FALSE}
library(survey)
data(api)
options(digits=2)
```

The survey package contains a census dataset of all California schools with at least 100 students, *apipop*, and several probability samples of the data.  We will use *apisrs*, a simple random sample without replacement of size 200, and *apistrat*, a stratified random sample of size 200. In these datasets, the primary attribute of interest is the Academic Performance Index.  Additional information about the schools and demographics of the schools are provided.  

For illustration purposes we will treat *apipop* as our finite population of interest and in various examples will use the samples *apisrs* and *apistrat*.
 We will assume that the attributes of interest are only known for the sample but that the auxiliary data are known for each school.  

#What do we make use of this sentence?
Assume we want to estimate ??? in insert two quantities based on api and sch.wide or comp.imp.



## Survey Estimators

This section presents guidance on when each estimator is most appropriate and shows how to fit the estimators using mase.

### Horvitz-Thompson Estimator

If no auxiliary data are available, then $\hat{y}_i = 0$ in the model-assisted estimator of $t_y$ and the estimator simplifies to
$$
\hat{t}_y = \sum_{i \in s} \frac{y_i}{\pi_i},
$$
the survey-weighted sum of the sampled values [@hor52].  To give an estimate of the average score, we use the HT estimator in the following manner:

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1))
ht_srs

ht_strat <- horvitzThompson(y = apistrat$api00, pi = apistrat$pw^(-1))
ht_strat
```

Then the Horvitz-Thompsons estimator for the average score is `r ht_srs$pop_mean` for the simple random sample and `r ht_strat$pop_mean` for the stratified sample. Since $N$, the population size, was not specified, it was assumed to be $\sum_{i \in s} \pi_i^{-1}$.
Note that we specify the inclusion probabilities with argument "pi".  If "pi" is unavailable or not supplied, the function would require the total population size $N$, and assume simple random sampling. 
```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, N = length(apisrs$api00))
ht_srs
#Is this output actually correct? 
```

The variance estimator can be found by adding the argument `var_est = TRUE` and specifying a method with `var_method`.

```{r}
ht_srs <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")
```

For the simple random sample, the variance of the HT estimator of the mean score is `r ht_srs$pop_mean_var`.  If the joint inclusions probabilites are known and positive (argument `pi2`), then the exact Horvitz-Thompson estimator of other single stage sampling designs can be found.  The variance can be estimated by selected one of the with-replacement variance estimators. 

An introduction to the various variance estimators is given at the last section of this documentaiton. 

```{r}
ht_strat <- horvitzThompson(y = apisrs$api00, pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HB")
```

### Post-stratified Estimator 


It is not uncommon for us to have categorical auxilary data that might give information we would like to use in modeling. It has been shown that such information can often increase the precision of our estimates by reducing variance. For instance, that a particular location is covered by forest would give good predictor of canopy cover since the plots of land labelled "forest"" is much more likely to be covered by canopy than those that are labelled "non-forest".

For instance, suppose we want to estimate $\mu_y$, where $y_i$ denotes the percentage of canopy cover for unit $i$ and we have a single categorical $x$ variable with known number of Categories.Then, assuming the group mean model  
$$
y_i = \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i, 
$$
the generalized regression estimator simplifies to the post-stratified estimator
$$
%Fix the upper bound of the first summand (it's not 2; what is it?)
\hat{\mu_j} = \frac{1}{N} \sum _{j=1}^J \frac{N_j}{\hat{N}_j} \sum _{i \in S_j} \frac{y_i}{\pi_i},
$$
where J, denoting the number of categories, in this example equals 2, and  $S_j $ refers to the set of observations. 

In particular, we note that the quantity $\frac{1}{\hat{N}_j} \sum _{i \in S_j} \frac{y_i}{\pi_i}$ is $\hat{\mu}_{hj}$, the estimated mean of stratum $j$ of the quantity of interest, $y$. 



In our example dataset, the binary categorical variable `awards` specifies whether a school is eligible for an awards program.   From the graph below, it appears that eligible schools have, on average, a higher api score.

```{r, echo=FALSE}
library(ggplot2)
ggplot(apistrat, aes(x = awards, y = api00)) + geom_boxplot()+ labs(x="Eligible for Awards Program?", y="API Scores for 2000") + theme_bw() 
```

To add the awards information, the estimator requires the sample values (`xsample`) and the population data (`xpop`). The following R scripts show three ways of achieving this, corresponding to three common forms in which we might be given the data. The first form of data we might be given is a detailed list of the state of all observations of the categorical variables describing whether they belong to this category. This data form is called "raw" data, and the following code shows how we can perform relevant computation.

```{r, message=FALSE}

#Raw data
#This method does not need to first extract population totals or population mean 
#(I need to explain why this method is sometimes unavailable. When do people not use this method?)
ps_srs_r <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = apipop$awards, datatype = "raw", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "bootstrapSRS")
```
The second form in which data often given is in terms of counts of observations in one cateogry and the total of those not in this category. The code will first produce data in such a format, stored in "pop_totals", and then perform computation based on this given information.

Now suppose that we don't have the raw data; we can achieve this through other information that is often provided
```{r, message=FALSE}
#Totals in each category 
#First computes the total number of awards, thus mimicking the form of given totals
pop_totals <- data.frame(table(apipop$awards))
ps_srs_t <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_totals, datatype = "totals", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HTSRS")
```

The third form of data is one where we have the proportion, or the percetange of observations in or out of each category. Again, the first line of code gives such a summary of data, and computation is performed by code given on the second line. 
```{r, message=FALSE}
#Means in each category
#First computes the proportion of the two categories
pop_means <- data.frame(table(apipop$awards)/length(apipop$awards))
ps_srs_m <- postStrat(y = apisrs$api00, xsample = apisrs$awards, xpop = pop_means, datatype = "means", pi = apisrs$pw^(-1), var_est = TRUE, var_method = "HT")
```


The above gives the estimated mean of api score for 2000,`r ps_srs_r$pop_mean`, assuming simple random sampling. The function unique to the post-stratification estimator is that it can give means according to different groups, a feature sometimes helpful but not available through other estimators. (Post-strat can give out means by groups, unique to post-Strat)

### Ratio Estimator
#Need to talk about this a bit more!
The ratio estimator is an estimator building on the Horvitz Thompson estimator. Suppose that this ratio has average $\beta $ that stays fairly constant throughout our samples. Then we can give the model 
$$
y_i = \beta x_i + \epsilon_i,
%discuss why this estimator has a quantitative auxilary variable, why this model makes sense, and talk about the assumption that we know \mu_x?
$$


where the errors $\epsilon_i$ would structurally have variance $\sigma ^2 x_i$ and mean at 0.We would assume a linear relationship between $x$ and $y$ through the origin, regardless of whether that relationship is positive or negative. 


In light of our HT estimator of $\mu_y$ and of $\mu_x$, we would have our estimated
$$
\hat{\beta}= \frac{\hat {\mu}_{y,ht}}{\hat{\mu}_{x,HT}}.
$$
It can then be shown from GREG estimator below that 
$$
\hat{\mu_y} = \frac{\mu_x}{\hat{\mu}_{x, HT}} \hat{\mu}_{y,ht,}
%Does the ratio estimator assume a positive relationship to work?
$$

to which we give the name ratio estimator. @sar92. We can see that this estimator builds on the Horvitz Thompson estimator because it scales the Horvitz Thompson estimate of mean of the desired quantity by a ratio of the true population mean of the predictor and its corresponding HT estimate.


We should nevertheless note a couple of caveats for the ratio estimator. Ratio Estimators are often biased. (It assumes regression through the origin which might contribute to the bias.)

Ratio estimation is most appropriate if a straight line through the origin summarizes the relationship between xi and yi and if the variance of yi about the line is proportional to xi. This assumption is often quite difficult to meet; even in simple random sampling where we already have that the ratio between yi and xi are constant, very little can we say about whether the regression line between these two variables traverse the origin.

We now give examples of using the ratio estimator to compute population mean.


We first perform some data exploration.
```{r}
explor_quant <- apisrs[,c(12, 20:23)]
pairs(explor_quant)
```
From this plot above, it seems that a good predictor for api00 is meals, since the negative linear correlation seems strong. 
```{r, message=FALSE}

#Raw data
#Just need to feed in one variable!!
re_srs_raw <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = apipop$meals, datatype = "raw", pi = NULL, N = NULL, pi2= NULL, var_est = TRUE, var_method = "HTSRS", B = 1000)
re_srs_raw
pop_means <- mean(apipop$meals)
re_srs_means <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_means, datatype = "means", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "HB", B = 1000)
re_srs_means
pop_totals <- sum(apipop$meals)
re_srs_totals <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_totals, datatype = "totals", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "bootstrapSRS", B = 1000)
re_srs_totals
```
Note that the three estimates agree completely with each other, as we should expect. We can modify our method according to the layout of our dataset; we can change which variance estimator to use in the "var" option.
### Generalized Regression Estimator 
The generalized regression estimator (GREG) can be used when the auxiliary data includes a mixture of both quantitative and categorical variables. It is expressed in the following form:
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}-\hat{m}(\boldsymbol{x}_{i})}{\pi_{i}} 
                + \sum_{i \in U}\hat{m}(\boldsymbol{x}_{i}) \right)
$$
where $\boldsymbol{x}_{i}$ is the auxiliary data, $\hat{m}(\boldsymbol{x}_{i})$ is the predicted value of $y$ given the auxiliary data, $y_{i}$ is the observed, $\pi_{i}$ is the inclusion probability, and $N$ is the size of the finite population. 

The exact form of the GREG is flexible. It varies depending on whether $y$ is quantitative or categorical and the availablity and relevancy of auxiliary data. Note that all aforementioned estimators are variations of the GREG. For example, in the case that no relevant auxiliary data exist, the $\hat{m}(x_{i})$ portions of our GREG equation simply equal zero, resulting in our Horvitz-Thompson estimator: 
$$
\hat{\mu_{y}} = \frac{1}{N} \left(\sum_{i \in s}     
                \frac{y_{i}}{\pi_{i}} \right)
$$


When $y$ is quantitative, GREG is called a linear regression as follows:
$$
y_{i} = \beta_{o} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + ... + \beta_{p}x_{ip} + 
        \epsilon_{i} \\
      = \boldsymbol{x}_{i}^T \boldsymbol{\beta} + \epsilon_{i}
$$

The following code calculates GREG for the quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
greg_linear_srs <- greg(y = apisrs$api00, 
                        xsample = apisrs[c("col.grad", "awards")], 
                        xpop = apipop[c("col.grad", "awards")], 
                        pi = apisrs$pw^(-1), model = "linear",
                        var_est = TRUE)
greg_linear_srs

# GREG applied to stratified sample
greg_linear_strat <- greg(y = apistrat$api00, 
                          xsample = apistrat[c("col.grad", "awards")],
                          xpop = apipop[c("col.grad", "awards")],
                          pi = apistrat$pw^(-1), model = "linear",
                          var_est = TRUE)
greg_linear_strat
```

When $y$ is categorical, GREG is called a logistic regression as follows:
$$
P(y_{i} = 1|\boldsymbol{x}_{i}) = \frac{exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
                                  {1+exp(\boldsymbol{x}_{i}^T \boldsymbol{\beta})}
$$

The logistic model results in probabilities between 0 and 1. The following code calculates GREG for the categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package:
```{r}
# GREG applied to simple random sample
# categorical with more than two categories, emit warning message, return 

# For categorical variables, we need to convert yes/no to 1/0. In our logistic regression, our categorical variables are "awards" and "sch.wide." We need to convert these columns into 1/0s in both the srs, strat, and the population 

######### SRS #########
# Creating a new column to contain 1/0s
apisrs[c("new_sch.wide")] <- NA
apisrs[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:200) {
  if(apisrs$sch.wide[i] == "Yes" ) {
    apisrs$new_sch.wide[i] <- 1
  } else if (apisrs$sch.wide[i] == "No") {
    apisrs$new_sch.wide[i] <- 0
  } 
}

# Converting "awards" from yes/no to 1/0
for (i in 1:200) {
  if (apisrs$awards[i] == "Yes") {
    apisrs$new_awards[i] <- 1
  } else if (apisrs$awards[i] == "No") {
    apisrs$new_awards[i] <- 0
  }
}

######### STRAT #########
# Creating a new column to contain 1/0s
apistrat[c("new_sch.wide")] <- NA
apistrat[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:200) {
  if(apistrat$sch.wide[i] == "Yes") {
    apistrat$new_sch.wide[i] <- 1
  } else if (apistrat$sch.wide[i] == "No") {
    apistrat$new_sch.wide[i] <- 0
  }
}

# Converting "awards" from yes/no to 1/0
for (i in 1:200) {
  if(apistrat$awards[i] == "Yes") {
    apistrat$new_awards[i] <- 1
  } else if (apistrat$awards[i] == "No") {
    apistrat$new_awards[i] <- 0
  }
}

######### POPULATION #########
# Creating a new column to contain 1/0s
apipop[c("new_sch.wide")] <- NA
apipop[c("new_awards")] <- NA

# Converting "sch.wide" from yes/no to 1/0
for (i in 1:6194) {
  if(apipop$sch.wide[i] == "Yes") {
    apipop$new_sch.wide[i] <- 1
  } else if (apipop$sch.wide[i] == "No") {
    apipop$new_sch.wide[i] <- 0
  }
}

# Converting "awards" from yes/no to 1/0
for (i in 1:6194) {
  if(apipop$awards[i] == "Yes") {
    apipop$new_awards[i] <- 1
  } else if (apipop$awards[i] == "No") {
    apipop$new_awards[i] <- 0
  }
}

######### GREG Calculations #########
# GREG applied to simple random sample
greg_logistic_srs <- greg(y = apisrs$new_awards, 
                          xsample = apisrs[c("col.grad", "new_sch.wide")],
                          xpop = apipop[c("col.grad", "new_sch.wide")], 
                          pi = apisrs$pw^(-1), model = "logistic")
greg_logistic_srs

# GREG applied to stratified sample
greg_logistic_strat <- greg(y = apistrat$new_awards, 
                            xsample = apistrat[c("col.grad", "sch.wide")],
                            xpop = apipop[c("col.grad", "sch.wide")],
                            pi = apistrat$pw^(-1), model = "logistic")
greg_logistic_strat
```

### Elastic Net Regression Estimator
The Elastic Net Regression estimator is simply the GREG estimator above, with an added condition in the original formula which uses a penalized survey-weighted least squares to incorporate model selection when estimating our coefficients. This added condition is called the "elastic net." 
$$
\hat{\boldsymbol{\beta}_{s}} = \underset{\boldsymbol{\beta}}{\text{arg min}} 
                              \left \{\sum_{i \in s}
                              \frac{(y_{i}-\boldsymbol{x}_{i}^T \boldsymbol{\beta})^2}
                              {\pi_{i}\sigma^2_{i}} + \lambda
                              \left[\alpha\sum_{j=1}^p \left|\beta_{j} \right| + 
                              (1-\alpha)\sum_{j=1}^p \beta^2_{j} \right]
                              \right \}
$$
Observe that the first half of the equation is the original GREG formula. The second half of the equation is the aforementioned "elastic net" condition, which is the penalty calculation.The elastic net itself is split into two halves. The first half of the elastic net, $\alpha\sum_{j=1}^p \left|\beta_{j} \right|$, is known as the LASSO penalty, which shrinks the amount of unnecessary variables. The second half of the elastic net, $(1-\alpha)\sum_{j=1}^p \beta^2_{j}$, is the Ridge Regression penalty, which is good for multicollinearity. The alpha value decides the relative amount weight to assign to each type of penalty. 

At this point, one might wonder - why add a penalty to the original GREG formula? Why create an elastic net regression estimator? What is "better" about the elastic net regression estimator than the generalized regression estimator? The GREG estimator does not guard against the presence of too many auxiliary variables, which may result in added variability. With the addition of the elastic net condition, the elastic net regression estimator shrinks the coefficients towards zero, therefore resulting in a better variability than the GREG. To combat the presence of superfluous auxiliary data, one could also choose subsets on which to perform GREG, but this process is tedious and time consuming, as well as inefficient. Imagine how many combinations can be made from n choose 2, n choose 3, n choose 4, etc. number of subsets! Therefore, by adding an elastic net condition to our original GREG estimator, we allow the penalization condition to "choose" the important $\beta$'s for us. 

Likewise with GREG, one can calculate linear and logistic regressions using the Elastic Net estimator. The following code calculates the elastic net regression for our quantitative $y$ variable, api, in our srs and strat sample datasets using the mase package.
```{r}
# Linear Elastic Net applied to simple random sample
# give long x vector to show how elastic net will "discard" certain variables
gregElastic_lin_srs <- gregElasticNet(y = apisrs$api00, 
                                      xsample = apisrs[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "sch.wide")],
                                      xpop = apipop[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_srs

# Linear Elastic Net applied to stratified sample
gregElastic_lin_strat <- gregElasticNet(y = apistrat$api00, 
                                      xsample = apistrat[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "sch.wide")],
                                      xpop = apipop[c("col.grad", "awards", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "sch.wide")], 
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "linear", var_est = TRUE)
gregElastic_lin_strat
```



The following code calculates the elastic net regression for our categorical $y$ variable, awards, in our srs and strat sample datasets using the mase package.
```{r}
# Logistic Elastic Net applied to simple random sample
gregElastic_log_srs <- gregElasticNet(y = apisrs$new_awards, 
                                      xsample = apisrs[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "new_sch.wide")],
                                      xpop = apipop[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "new_sch.wide")], 
                                      pi = apisrs$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_srs

# Logistic Elastic Net applied to stratified sample
gregElastic_log_strat <- gregElasticNet(y = apistrat$new_awards, 
                                      xsample = apistrat[c("col.grad",  
                                                         "snum", "dnum",
                                                         "cnum", "pcttest",
                                                         "meals", "new_sch.wide")],
                                      xpop = apipop[c("col.grad", 
                                                         "snum", "dnum",
                                                         "cnum", "pcttest", 
                                                         "meals", "new_sch.wide")], 
                                      pi = apistrat$pw^(-1), alpha = 0.5, 
                                      model = "logistic", var_est = TRUE)
gregElastic_log_strat
```


##Variance Estimation

To compute confidence intervals, we need information on the variance of our various estimators. Yet often the exact variance formulas require quantities that are often not given by available data or could only be achieved by a computationally expensive route. Therefore we make use of estimators for these variance computations. 

Below we discuss relevant estimators for variances, suitable scenarios to use them, and examples of applying them to a real-world data set. The relevant functions are included in the package $mase$.

#Variance  Estimator for Horvitz-Thompson (HT) Estimator 

Horvitz-Thompson Estimator has a wide range of applicable scenarios and is often a "go-to" estimator. Yet its variance estimator, despite its ability converge to the true variance in the large sample limit, has the unfortunate requirement of joint inclusion probabilities--the probability that two elements together are included in our sample ($\pi_{ij}$, for different values of $i \neq j$). If this quantity is available, then the estimator could be directly applied to give an estimated variance that will converge to the true variance given a large enough sample.

In reality this joint probability is often difficult to acquire. There are two possible scenarios given the lack of joint probabilities. In the case where the data comes from simple random sampling(SRS), we could use variance estimators for \textbf{HTSRS} or \textbf{Bootstrap} to achieve an estimated variance. These methods will be discussed below. In the case where simple random sampling is not assumed, and given we don't have joint probabilities, we recommend using Hansen-Hurwitz (HH) or Hajet-Berger estimators.

Notice that regardless of whether we have joint probabilities or assume simple random sampling, the estimator will still give an estimated total number. The limitation is that it won't give a variance estimate.

#Variance Estimators for HTSRS and Bootstrap Assuming SRS

In the case where we have simple random sample, both methods will give an estimate for variance. In the case where an HT estimator is used, we recommend using the variance estimate for HTSRS. If not, we recommend using Bootstrap variance estimator. Notice that the bootstrap estimator as we give in this package assumes simple random sampling. re_srs_raw <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = apipop$meals, datatype = "raw", pi = NULL, N = NULL, pi2= NULL, var_est = TRUE, var_method = "HTSRS", B = 1000)
```{r, eval = FALSE}

pop_totals <- sum(apipop$meals)
re_srs_totals <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_totals, datatype = "totals", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "bootstrapSRS", B = 1000)
re_srs_totals


re_srs_totals <- ratioEstimator(y = apisrs$api00, xsample = apisrs$meals, xpop = pop_totals, datatype = "totals", pi = NULL, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "HTSRS", B = 1000)
re_srs_totals
```
#Variance Estimators for Hajek-Berger and Hansen-Hurwitz

In the case where joint inclusion probabilities are unavailable and the sampling structure is not simple random sampling, we recommend using these estimators and their variance estiamtes. These variance estimators bypass the problem of lack of joint inclusion probabilities by approximating with replacement. The downside is that the variance estimates will be larger than those with joint inclusion probability is known, but in the large sample limit the estimates will approach true variance.  

In the example, if $pi$ is given the command "NULL", the estimators will assume simple random sampling. If the dataset is not using simple random sampling, a weight will be given, and $pi = 1/ \text{weight}$.

```{r}
re_srs_means_HB <- ratioEstimator(y = apistrat$api00, xsample = apistrat$meals, xpop = pop_means, datatype = "means", pi = 1/apistrat$pw , N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "HB")

re_srs_means_HH <- ratioEstimator(y = apistrat$api00, xsample = apistrat$meals, xpop = pop_means, datatype = "means", pi = 1/apistrat$pw, N = dim(apipop)[1], pi2= NULL, var_est = TRUE, var_method = "HH")

```


